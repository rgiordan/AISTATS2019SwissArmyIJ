{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Refit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we calculate the parameters used for exact CV by refitting the model initially fit in step one, the notebook ``fit_model_and_save``.\n",
    "\n",
    "For expository purposes this notebook calculates the refit for only one weight vector.  To compute exact CV, one would perform the corresponding computation for all leave-k-out weight vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "np.random.seed(3452453)\n",
    "\n",
    "import paragami\n",
    "\n",
    "from aistats2019_ij_paper import regression_mixture_lib as rm_lib\n",
    "from aistats2019_ij_paper import saving_gmm_utils\n",
    "from aistats2019_ij_paper import mse_utils\n",
    "\n",
    "import plot_utils_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FitDerivatives.\n",
      "Using provided t_jac.\n",
      "Using provided full_hess.\n"
     ]
    }
   ],
   "source": [
    "# Load the initial fit.\n",
    "# This file was produced by the notebook ``fit_model_and_save``.\n",
    "initial_fit_infile = '../fits/initial_fit.npz'\n",
    "full_fit, gmm, regs, metadata = \\\n",
    "    saving_gmm_utils.load_initial_optimum(initial_fit_infile)\n",
    "timepoints = metadata['timepoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, choose some timepoints to leave out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate passing arguments in on the command line.\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.num_times = 1\n",
    "args.which_comb = 1\n",
    "args.max_num_timepoints = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "args.max_num_timepoints": "7",
     "args.num_times": "1"
    }
   },
   "source": [
    "The number of points left out (that is, $k$) is given by ``num_times``, which is {{args.num_times}}.  The largest timepoint we leave out is given by ``max_num_timepoints``, which is {{args.max_num_timepoints}}.  Because later timepoints are not affected by the smoothing, there is no reason to leave them out.  \n",
    "\n",
    "There are a certain number of ways to leave $k$ out of {{args.max_num_timepoints}} timepoints, and ``which_comb`` chooses one of them in the order given by the function ``itertools.combinations``.  Of course, when $k=1$, ``which_comb`` simply chooses which timepoint to leave out.  ``mse_utils.get_indexed_combination`` maps ``which_comb`` to particular timepoints in a consistent way.\n",
    "\n",
    "Full exact CV would run this script for all {{args.max_num_timepoints}} choose $k$ values of ``which_comb``.\n",
    "\n",
    "Because we have repeated measurements at each timepoint, leaving out a single timepoint will correspond to leaving out multiple row of the observation matrix.  Those rows are determined by ``mse_utils.get_time_weight``, which also returns a weight vector setting these observations' weights to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left out timepoint: [1]\n",
      "Left out observations: [3 4 5]\n",
      "Leave-k-out weights: [1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "lo_inds = mse_utils.get_indexed_combination(\n",
    "    num_times=args.num_times, which_comb=args.which_comb,\n",
    "    max_num_timepoints=args.max_num_timepoints)\n",
    "new_time_w, full_lo_inds = mse_utils.get_time_weight(lo_inds, timepoints)\n",
    "\n",
    "print('Left out timepoint: {}'.format(lo_inds))\n",
    "print('Left out observations: {}'.format(full_lo_inds))\n",
    "print('Leave-k-out weights: {}'.format(new_time_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now re-optimize with the new weights.\n",
    "\n",
    "Note that we could either start the optimization at the initial optimum (a \"warm start\") or do a fresh start from k-means.  A fresh start is more time consuming but a more stringent test for the accuracy of the IJ.  We calculate both, but report results from the fresh start in the paper.  In the notebook ``examine_and_save_results``, you can choose to examine either set of results.\n",
    "\n",
    "Here, for consistency with the paper, we re-initialize with k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: f = -153.38003431\n",
      "Iter 1: f = -152.49438715\n",
      "Iter 2: f = -153.69147895\n",
      "Iter 3: f = -153.83779915\n",
      "Iter 4: f = -154.02397812\n",
      "Iter 5: f = -153.41393391\n",
      "Iter 6: f = -154.10396420\n",
      "Iter 7: f = -154.14366282\n",
      "Iter 8: f = -154.14261201\n",
      "Iter 9: f = -154.16417745\n",
      "Iter 10: f = -154.18307547\n",
      "Iter 11: f = -154.20711481\n",
      "Iter 12: f = -154.22118064\n",
      "Iter 13: f = -154.27402715\n",
      "Iter 14: f = -154.28739474\n",
      "Iter 15: f = -154.33849929\n",
      "Iter 16: f = -154.03580241\n",
      "Iter 17: f = -154.35421130\n",
      "Iter 18: f = -154.36910489\n",
      "Iter 19: f = -154.36872458\n",
      "Iter 20: f = -154.37238982\n",
      "Iter 21: f = -154.37722095\n",
      "Iter 22: f = -154.38186985\n",
      "Iter 23: f = -154.38410992\n",
      "\tUpdating preconditioner...\n",
      "\tRunning preconditioned optimization...\n",
      "Preconditioned iteration 1\n",
      "  Running preconditioned optimization.\n",
      "Iter 0: f = -154.38410992\n",
      "Iter 1: f = -154.38423176\n",
      "Iter 2: f = -154.38584092\n",
      "Iter 3: f = -154.21889674\n",
      "Iter 4: f = -154.42200228\n",
      "Iter 5: f = -154.39603234\n",
      "Iter 6: f = -154.39957947\n",
      "Iter 7: f = -154.41374585\n",
      "Iter 8: f = -154.43397491\n",
      "Iter 9: f = -154.43484046\n",
      "Iter 10: f = -154.43484816\n",
      "Iter 11: f = -154.43484816\n",
      "Preconditioned iteration 2\n",
      "  Getting Hessian and preconditioner.\n",
      "  Running preconditioned optimization.\n",
      "Iter 12: f = -154.43484816\n",
      "Iter 13: f = -154.43484816\n",
      "Converged.\n",
      "Optimization terminated successfully.\n",
      "Refit time: 24.85831880569458 seconds\n"
     ]
    }
   ],
   "source": [
    "regs.time_w = deepcopy(new_time_w)\n",
    "reg_params_w = regs.get_optimal_regression_params()\n",
    "gmm.set_regression_params(reg_params_w)\n",
    "\n",
    "init_gmm_params = \\\n",
    "    rm_lib.kmeans_init(gmm.transformed_reg_params,\n",
    "                       gmm.num_components, 50)\n",
    "init_x = gmm.gmm_params_pattern.flatten(init_gmm_params, free=True)\n",
    "\n",
    "opt_time = time.time()\n",
    "gmm_opt, init_x2 = gmm.optimize(init_x, gtol=1e-2)\n",
    "\n",
    "print('\\tUpdating preconditioner...')\n",
    "kl_hess = gmm.update_preconditioner(init_x2)\n",
    "\n",
    "print('\\tRunning preconditioned optimization...')\n",
    "gmm.conditioned_obj.reset()\n",
    "reopt, gmm_params_free_w = gmm.optimize_fully(init_x2, verbose=True)\n",
    "print(gmm_opt.message)\n",
    "opt_time = time.time() - opt_time\n",
    "\n",
    "print('Refit time: {} seconds'.format(opt_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm_params_w = \\\n",
    "    full_fit.comb_params_pattern['mix'].fold(\n",
    "        gmm_params_free_w, free=True)\n",
    "refit_comb_params = {\n",
    "    'mix': gmm_params_w,\n",
    "    'reg': reg_params_w }\n",
    "refit_comb_params_free = \\\n",
    "    full_fit.comb_params_pattern.flatten(refit_comb_params, free=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ../fits/refit__num_times1__which_comb1.npz\n"
     ]
    }
   ],
   "source": [
    "save_filename = \\\n",
    "    '../fits/refit__num_times{}__which_comb{}.npz'.format(\n",
    "            args.num_times, args.which_comb)\n",
    "print('Saving to {}'.format(save_filename))\n",
    "saving_gmm_utils.save_refit(\n",
    "    outfile=save_filename,\n",
    "    comb_params_free=refit_comb_params_free,\n",
    "    comb_params_pattern=full_fit.comb_params_pattern,\n",
    "    initial_fit_infile=initial_fit_infile,\n",
    "    time_w=new_time_w,\n",
    "    lo_inds=lo_inds,\n",
    "    full_lo_inds=full_lo_inds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistats2019_ij_paper",
   "language": "python",
   "name": "aistats2019_ij_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
