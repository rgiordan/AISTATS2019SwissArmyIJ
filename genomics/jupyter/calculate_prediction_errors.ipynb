{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate the IJ and prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, for a single weight vector, we calculate the IJ itself as well as the prediction errors for exact CV and IJ.  This notebook uses the output of the notebooks ``load_and_refit`` and ``fit_model_and_save``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paragami\n",
    "import vittles\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(3452453)\n",
    "\n",
    "from aistats2019_ij_paper import regression_lib as reg_lib\n",
    "from aistats2019_ij_paper import sensitivity_lib as sens_lib\n",
    "from aistats2019_ij_paper import saving_gmm_utils\n",
    "from aistats2019_ij_paper import mse_utils\n",
    "\n",
    "import plot_utils_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate passing arguments in on the command line.\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "args = Args()\n",
    "args.num_times = 1\n",
    "args.which_comb = 1\n",
    "args.max_num_timepoints = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original fit.\n",
      "Initializing FitDerivatives.\n",
      "Using provided t_jac.\n",
      "Using provided full_hess.\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Load the original fit.\n",
    "\n",
    "print('Loading original fit.')\n",
    "initial_fit_infile = '../fits/initial_fit.npz'\n",
    "full_fit, gmm, regs, initial_metadata = \\\n",
    "    saving_gmm_utils.load_initial_optimum(initial_fit_infile)\n",
    "\n",
    "opt_comb_params = full_fit.get_comb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Load the test data\n",
    "\n",
    "test_regression_infile = '../fits/test_regressions.json'\n",
    "with open(test_regression_infile) as infile:\n",
    "    regs_test = reg_lib.Regressions.from_json(infile.read())\n",
    "\n",
    "##########################################\n",
    "# Load a refit as specfified by ``args``.\n",
    "\n",
    "refit_filename = \\\n",
    "    '../fits/refit__num_times{}__which_comb{}.npz'.format(\n",
    "            args.num_times, args.which_comb)\n",
    "\n",
    "comb_params_free_refit, comb_params_pattern_refit, refit_metadata = \\\n",
    "    saving_gmm_utils.load_refit(refit_filename)\n",
    "\n",
    "time_w = refit_metadata['time_w']\n",
    "lo_inds = refit_metadata['lo_inds']\n",
    "full_lo_inds = refit_metadata['full_lo_inds']\n",
    "\n",
    "assert(comb_params_pattern_refit == full_fit.comb_params_pattern)\n",
    "comb_params_refit = comb_params_pattern_refit.fold(\n",
    "    comb_params_free_refit, free=True)\n",
    "\n",
    "time_w = refit_metadata['time_w']\n",
    "lo_inds = refit_metadata['lo_inds']\n",
    "full_lo_inds = refit_metadata['full_lo_inds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects named ``comb_params`` refer to both the regression and clustering parameters.  The name ``free`` refers to the unconstrained flat value for the parameters as calculated by ``paragami``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Regression pattern: ',\n",
    "      comb_params_pattern_refit['reg'])\n",
    "\n",
    "print('Clustering pattern: ',\n",
    "      comb_params_pattern_refit['mix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the infinitesimal jackknife."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``vittles`` package makes it easy to calculate linear approximations to the sensitivity of M-estimators to hyperparameters, of which the IJ is a special case.  Here, the ``HyperparameterSensitivityLinearApproximation`` uses the sparse value of $H_1$ calculated earlier.\n",
    "\n",
    "Note that $H_1$ is factorized during the initialization of ``weight_sens``, and that it takes relatively little time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that if you don't cast the jacobian to a numpy array from\n",
    "# a numpy matrix, the output is a 2d-array, causing confusion later.\n",
    "weight_sens = vittles.HyperparameterSensitivityLinearApproximation(\n",
    "    objective_fun=lambda: 0,\n",
    "    opt_par_value=full_fit.comb_params_free,\n",
    "    hyper_par_value=regs.time_w,\n",
    "    hessian_at_opt=sp.sparse.csc_matrix(full_fit.full_hess),\n",
    "    cross_hess_at_opt=np.array(full_fit.t_jac.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the ``weight_sens`` object to approximate the \"free\" value of the combined parameters at ``time_w``.  The IJ operates in unconstrained space, so we use ``paragami`` to fold the unconstrained vector back into a dictionary of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the infinitesimal jackknife for the refit weight vector.\n",
    "lr_time = time.time()\n",
    "comb_params_free_lin = \\\n",
    "    weight_sens.predict_opt_par_from_hyper_par(time_w)\n",
    "lr_time = time.time() - lr_time\n",
    "print('Infinitesimal jackknife time: {}'.format(lr_time))\n",
    "\n",
    "comb_params_lin = full_fit.comb_params_pattern.fold(comb_params_free_lin, free=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate various prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the prediction error is the difference between the data and the posterior expected cluster centroid for a particular gene.  Let us consider the original optimal clustering parameters, ``opt_comb_params['mix']``.  To get the test set error on gene $g$ for these parameters, we need to do the following steps:\n",
    "\n",
    "1. Run the regression for gene $g$ in the test set\n",
    "2. Classify the regression, calculating $\\mathbb{E}_{q^*_z}[z_{g}]$.  This is a function of the clustering parameters and the regression line for gene $g$.\n",
    "3. Calculate the expected posterior cluster centroid for gene $g$, which is $\\mu_g^* = \\sum_k \\mathbb{E}_{q^*_z}[z_{gk}] \\mu_k$.\n",
    "4. Because the transformation discards the mean information, compare the de-meaned data to the estimated centroid: $error_{gt} = \\left(y_{gt} - \\frac{1}{T}\\sum_{t'=1}^{T} y_{gt'}\\right) - \\mu_{gt}^*$.\n",
    "\n",
    "Note that step one could re-run the regression either with the original weights or the new weights.  We found that this decision does not matter qualitatively.  Here and in the paper, we simply classify the original regression, but the notebook ``examine_and_save_results`` can produce results for oth the original and re-weighted regressions.\n",
    "\n",
    "We will examine prediction error on the time points that are left out, that is, for observations in ``full_lo_inds``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating prediction error.')\n",
    "\n",
    "# Get the training set error on the full data.\n",
    "train_error = mse_utils.get_lo_err_folded(\n",
    "    opt_comb_params,\n",
    "    keep_inds=full_lo_inds,\n",
    "    mse_regs=regs,\n",
    "    mse_reg_params=opt_comb_params['reg'],\n",
    "    gmm=gmm)\n",
    "\n",
    "############\n",
    "# Original fit.\n",
    "\n",
    "# Get the optimal test set regressions.\n",
    "reg_params_test = regs_test.get_optimal_regression_params()\n",
    "\n",
    "# Get the test error for the original fit.\n",
    "orig_test_error = mse_utils.get_lo_err_folded(\n",
    "    opt_comb_params,\n",
    "    keep_inds=full_lo_inds,\n",
    "    mse_regs=regs_test,\n",
    "    mse_reg_params=reg_params_test,\n",
    "    gmm=gmm)\n",
    "\n",
    "orig_pred = mse_utils.get_predictions(\n",
    "    gmm, opt_comb_params['mix'], reg_params_test)\n",
    "\n",
    "# Get the test error for the CV refit.\n",
    "cv_error = mse_utils.get_lo_err_folded(\n",
    "    comb_params_refit,\n",
    "    keep_inds=full_lo_inds,\n",
    "    mse_regs=regs_test,\n",
    "    mse_reg_params=reg_params_test,\n",
    "    gmm=gmm)\n",
    "\n",
    "cv_pred = mse_utils.get_predictions(\n",
    "    gmm, comb_params_refit['mix'], reg_params_test)\n",
    "\n",
    "# Get the test error for the IJ approximation.\n",
    "ij_error = mse_utils.get_lo_err_folded(\n",
    "    comb_params_lin,\n",
    "    keep_inds=full_lo_inds,\n",
    "    mse_regs=regs_test,\n",
    "    mse_reg_params=reg_params_test,\n",
    "    gmm=gmm)\n",
    "\n",
    "ij_pred = mse_utils.get_predictions(\n",
    "    gmm, comb_params_lin['mix'], reg_params_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a cursory comparison of the results.  For a more detailed analysis, including the results that went into the paper, see the notebook ``examine_and_save_results``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_excess_error = cv_error - orig_test_error\n",
    "ij_excess_error = ij_error - orig_test_error\n",
    "\n",
    "def GetColDf(col):\n",
    "    return pd.DataFrame(\n",
    "        {'cv_error': cv_error[:, col],\n",
    "         'cv_excess': cv_excess_error[:, col],\n",
    "         'ij_error': ij_error[:, col],\n",
    "         'ij_excess': ij_excess_error[:, col],\n",
    "         'col': col})\n",
    "\n",
    "result = pd.concat([ GetColDf(col) for col in range(len(full_lo_inds)) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply look at the point-by-point error, CV and IJ are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='cv_error', y='ij_error', data=result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is because the error in each point is dominated by the error at the original optimum.  To meaningfully compare the IJ to CV, we should compare the difference between the IJ and CV error and the error at the original optimum.  The distribution of these \"difference-in-difference\" errors is shown in the next plot.\n",
    "\n",
    "Some clear outliers can be seen.  However, note that, in this case, overplotting makes IJ looks worse than it is -- in the histograms you can see that most differences are very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='cv_excess', y='ij_excess', data=result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect from a linear approximation, the IJ does the worst when the predicted change for CV is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misfit = np.max(np.abs(cv_excess_error - ij_excess_error), axis=1)\n",
    "abs_cv_excess_error = np.max(np.abs(cv_excess_error), axis=1) \n",
    "\n",
    "sns.jointplot(abs_cv_excess_error, misfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize some of the genes where IJ badly misestimates the CV error.  Clearly, in these cases, re-fitting with the left-out points (shown with large dots) produced large changes that the IJ did not capture.  In general, it appears that the IJ errs relative to CV by not moving far enough from the original optimum.\n",
    "\n",
    "Despite the poor fit on these extreme genes, we stress that most genes exhibited small changes in both CV and IJ.  For these genes, IJ performs well enough to capture salient aspects of the estimated out-of-sample error.  For more detailed analysis of this point, see the notebook ``examine_and_save_results``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timepoints = initial_metadata['timepoints']\n",
    "timepoints_stretch = np.sqrt(timepoints)\n",
    "\n",
    "def PlotGenePredictions(gene_ind):\n",
    "    _, figs = plt.subplots(1, 3, figsize=(15,6))\n",
    "    \n",
    "    for i in range(3):\n",
    "        np.random.seed(42)\n",
    "        plot_utils_lib.PlotRegressionLine(\n",
    "            timepoints_stretch, regs_test, reg_params_test, gene_ind, this_plot=figs[i])\n",
    "        figs[i].plot(timepoints_stretch[full_lo_inds],\n",
    "                     regs_test.y[gene_ind, full_lo_inds], 'o', markersize=10)\n",
    "\n",
    "    plot_utils_lib.PlotPredictionLine(\n",
    "        timepoints_stretch, regs_test, orig_pred, gene_ind, this_plot=figs[0])\n",
    "    figs[0].set_title('Gene {} original fit'.format(gene_ind))\n",
    "\n",
    "    plot_utils_lib.PlotPredictionLine(\n",
    "        timepoints_stretch, regs_test, ij_pred, gene_ind, this_plot=figs[1])\n",
    "    figs[1].set_title('Gene {} IJ fit'.format(gene_ind))\n",
    "\n",
    "    plot_utils_lib.PlotPredictionLine(\n",
    "        timepoints_stretch, regs_test, cv_pred, gene_ind, this_plot=figs[2])\n",
    "    figs[2].set_title('Gene {} CV fit'.format(gene_ind))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "worst_fits = np.argsort(-1 * misfit)\n",
    "\n",
    "for gene in worst_fits[0:5]:\n",
    "    PlotGenePredictions(gene)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistats2019_ij_paper",
   "language": "python",
   "name": "aistats2019_ij_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
